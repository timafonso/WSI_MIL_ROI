{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'models'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 27\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mstatistics\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[39m# ============================= Models and Datasets =================================\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m Attention, GatedAttention, AdditiveAttention, ModAdditiveAttention\n\u001b[1;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mwsi_datasets\u001b[39;00m \u001b[39mimport\u001b[39;00m tumor_collate, tumor_pad_collate_fn, gene_collate, gene_pad_collate_fn, GeneExpressionDataset\n\u001b[1;32m     30\u001b[0m \u001b[39m# ============================ TensorBoard and Logging =============================\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'models'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(\"..\")\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# ================================== Torch Imports ==================================\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data_utils\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# =================================== Metrics =======================================\n",
    "\n",
    "from torchmetrics.classification import BinaryAccuracy, BinaryRecall, BinaryF1Score\n",
    "from torcheval.metrics import BinaryAUROC, R2Score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "\n",
    "# ============================= Models and Datasets =================================\n",
    "\n",
    "from models.models import Attention, GatedAttention, AdditiveAttention, ModAdditiveAttention\n",
    "from wsi_datasets import tumor_collate, tumor_pad_collate_fn, gene_collate, gene_pad_collate_fn, GeneExpressionDataset\n",
    "\n",
    "# ============================ TensorBoard and Logging =============================\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import pickle\n",
    "\n",
    "#================================= Finetuning ======================================\n",
    "\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../runs/cross-val/gene_expression/BRCA_10x/baselines/Attention/trial_133_\n"
     ]
    }
   ],
   "source": [
    "#==================================== PARAMETERS ====================================================\n",
    "\n",
    "CUDA = True\n",
    "SEED = 5\n",
    "\n",
    "# --------------------------------- Hyperparameters ----------------------------------------\n",
    "\n",
    "LR = 0.000023422110057\n",
    "REG = 0.000152652510413\n",
    "NUM_EPOCHS = 35\n",
    "KFOLD_SPLITS = 5\n",
    "BATCH_SIZE = 32\n",
    "uniform = torch.nn.init.xavier_uniform_\n",
    "uni = \"xavier\" if uniform == torch.nn.init.xavier_uniform_ else \"kaiming\"\n",
    "\n",
    "# --------------------------------- Metrics Indexes -----------------------------------------\n",
    "\n",
    "LOSS_INDEX = 0\n",
    "ACCURACY_INDEX = 1\n",
    "AUC_INDEX = 2\n",
    "RECALL_INDEX = 3\n",
    "F1_INDEX = 4\n",
    "PERCENTAGE_ACCURACY_INDEX = 5\n",
    "\n",
    "# ------------------------------------- Labels ----------------------------------------------\n",
    "\n",
    "LABELS = [\"tumor\", \"tp53\"]\n",
    "\n",
    "# -------------------------------- Train/Test Split -----------------------------------------\n",
    "\n",
    "TRAIN_PERC = 0.8\n",
    "\n",
    "# -------------------------------------------------------------------------------------------\n",
    "\n",
    "EMBEDDINGS = True\n",
    "USE_TENSORBOARD = True\n",
    "\n",
    "DATASET_HDF5 = \"../datasets/gene_expression/BRCA_TP53_10x.hdf5\"\n",
    "CUDA_DEVICE = \"cuda:0\"\n",
    "\n",
    "TENSORBOARD_DIRECTORY = \"\"\n",
    "MODEL_WEIGHTS_FILE = \"\"\n",
    "TEST_SET_INDICES = 'test_indices.pkl'\n",
    "print(TENSORBOARD_DIRECTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPU is ON!\n"
     ]
    }
   ],
   "source": [
    "# ================================ Initializations ===============================================\n",
    "\n",
    "# CUDA initializations\n",
    "torch.cuda.init()\n",
    "torch.cuda.memory_summary(device=None, abbreviated=False)\n",
    "cuda = CUDA and torch.cuda.is_available()\n",
    "device = torch.device(CUDA_DEVICE)\n",
    "torch.manual_seed(SEED)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    print('\\nGPU is ON!')\n",
    "\n",
    "# Model to train\n",
    "model_type = Attention\n",
    "# Cross Validation\n",
    "splits = KFold(n_splits = KFOLD_SPLITS, shuffle=True, random_state=42)\n",
    "\n",
    "# Tensorboard Writer\n",
    "if USE_TENSORBOARD:\n",
    "    writer = SummaryWriter(TENSORBOARD_DIRECTORY)\n",
    "\n",
    "# Weights Initialization Function\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        uniform(m.weight)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================== Metrics ======================================================\n",
    "\n",
    "# -------------------------------- Metrics Intialization --------------------------\n",
    "\n",
    "auc_metric = BinaryAUROC().to(device)    \n",
    "accuracy_metric = BinaryAccuracy(threshold=0.5).to(device)\n",
    "recall_metric = BinaryRecall(threshold=0.5).to(device)\n",
    "F1_metric = BinaryF1Score(threshold=0.5).to(device)\n",
    "R2Score_metric = R2Score()\n",
    "\n",
    "# ----------------------------- Metrics Calculation Functions -----------------------\n",
    "\n",
    "def calculate_r2_score(probs, true_labels):\n",
    "    R2Score_metric.update(probs, true_labels)\n",
    "    r2score = R2Score_metric.compute()\n",
    "    R2Score_metric.reset()\n",
    "    return r2score\n",
    "\n",
    "def calculate_auc(probs, true_labels):\n",
    "    auc_metric.update(probs, true_labels)\n",
    "    auc = auc_metric.compute()\n",
    "    auc_metric.reset()\n",
    "    return auc\n",
    "\n",
    "def calculate_accuracy(probs, true_labels):\n",
    "    return accuracy_metric(probs, true_labels)\n",
    "\n",
    "def calculate_recall(probs, true_labels):\n",
    "    return recall_metric(probs, true_labels)\n",
    "\n",
    "def calculate_F1(probs, true_labels):\n",
    "    return F1_metric(probs, true_labels)\n",
    "\n",
    "\n",
    "def calculate_metrics(probs, true_labels):\n",
    "    probs = torch.FloatTensor(probs).to(device)\n",
    "    true_labels = torch.FloatTensor(true_labels).to(device)\n",
    "    \n",
    "    accuracy = calculate_accuracy(probs, true_labels)\n",
    "    auc = calculate_auc(probs, true_labels)\n",
    "    f1 = calculate_F1(probs, true_labels)\n",
    "    recall = calculate_recall(probs, true_labels)\n",
    "    \n",
    "    return accuracy, auc, f1, recall\n",
    "\n",
    "\n",
    "# --------------------------- Metrics Presentation Functions ---------------------\n",
    "\n",
    "def get_classification_report(probs, true_labels):\n",
    "    report = classification_report(true_labels, probs, target_names=[\"Negative\", \"Positive\"])\n",
    "    print(report)\n",
    "\n",
    "def metrics_to_tensorboard(fold_metrics):\n",
    "    if USE_TENSORBOARD:\n",
    "        for fold in range(fold_metrics.shape[0]):\n",
    "            for epoch in range(fold_metrics.shape[1]):\n",
    "                writer.add_scalar('{} Loss - Fold {}'.format(\"training\", fold), fold_metrics[fold, epoch, 0, LOSS_INDEX], epoch)\n",
    "                writer.add_scalar('{} Accuracy - Fold {}'.format(\"training\", fold), fold_metrics[fold, epoch, 0, ACCURACY_INDEX], epoch)\n",
    "                writer.add_scalar('{} AUC - Fold {}'.format(\"training\", fold), fold_metrics[fold, epoch, 0, AUC_INDEX], epoch)\n",
    "                writer.add_scalar('{} Recall - Fold {}'.format(\"training\", fold), fold_metrics[fold, epoch, 0, RECALL_INDEX], epoch)\n",
    "                writer.add_scalar('{} F1 - Fold {}'.format(\"training\", fold), fold_metrics[fold, epoch, 0, F1_INDEX], epoch)\n",
    "                \n",
    "                writer.add_scalar('{} Loss - Fold {}'.format(\"validation\", fold), fold_metrics[fold, epoch, 1, LOSS_INDEX], epoch)\n",
    "                writer.add_scalar('{} Accuracy - Fold {}'.format(\"validation\", fold), fold_metrics[fold, epoch, 1, ACCURACY_INDEX], epoch)\n",
    "                writer.add_scalar('{} AUC - Fold {}'.format(\"validation\", fold), fold_metrics[fold, epoch, 1, AUC_INDEX], epoch)\n",
    "                writer.add_scalar('{} Recall - Fold {}'.format(\"validation\", fold), fold_metrics[fold, epoch, 1, RECALL_INDEX], epoch)\n",
    "                writer.add_scalar('{} F1 - Fold {}'.format(\"validation\", fold), fold_metrics[fold, epoch, 1, F1_INDEX], epoch)\n",
    "                writer.add_scalar('{} Percentages - Fold {}'.format(\"validation\", fold), fold_metrics[fold, epoch, 1, PERCENTAGE_ACCURACY_INDEX], epoch) \n",
    "\n",
    "\n",
    "        for epoch in range(fold_metrics.shape[1]):\n",
    "            writer.add_scalar('{} Loss - Fold {}'.format(\"training\", \"avg\"), np.mean(fold_metrics[:, epoch, 0, LOSS_INDEX], axis=0), epoch)\n",
    "            writer.add_scalar('{} Accuracy - Fold {}'.format(\"training\", \"avg\"), np.mean(fold_metrics[:, epoch, 0, ACCURACY_INDEX], axis=0), epoch)\n",
    "            writer.add_scalar('{} AUC - Fold {}'.format(\"training\", \"avg\"), np.mean(fold_metrics[:, epoch, 0, AUC_INDEX], axis=0), epoch)\n",
    "            writer.add_scalar('{} Recall - Fold {}'.format(\"training\", \"avg\"), np.mean(fold_metrics[fold, epoch, 0, RECALL_INDEX]), epoch)\n",
    "            writer.add_scalar('{} F1 - Fold {}'.format(\"training\", \"avg\"), np.mean(fold_metrics[fold, epoch, 0, F1_INDEX]), epoch)   \n",
    "            \n",
    "            writer.add_scalar('{} Loss - Fold {}'.format(\"validation\", \"avg\"), np.mean(fold_metrics[:, epoch, 1, LOSS_INDEX], axis=0), epoch)\n",
    "            writer.add_scalar('{} Accuracy - Fold {}'.format(\"validation\", \"avg\"), np.mean(fold_metrics[:, epoch, 1, ACCURACY_INDEX], axis=0), epoch)\n",
    "            writer.add_scalar('{} AUC - Fold {}'.format(\"validation\", \"avg\"), np.mean(fold_metrics[:, epoch, 1, AUC_INDEX], axis=0), epoch)\n",
    "            writer.add_scalar('{} Recall - Fold {}'.format(\"validation\", \"avg\"), np.mean(fold_metrics[:, epoch, 1, RECALL_INDEX]), epoch)\n",
    "            writer.add_scalar('{} F1 - Fold {}'.format(\"validation\", \"avg\"), np.mean(fold_metrics[:, epoch, 1, F1_INDEX]), epoch) \n",
    "            writer.add_scalar('{} Percentages - Fold {}'.format(\"validation\", \"avg\"), np.mean(fold_metrics[:, epoch, 1, PERCENTAGE_ACCURACY_INDEX]), epoch)\n",
    "\n",
    "    return np.mean(fold_metrics[:, -1, 1, LOSS_INDEX]), np.mean(fold_metrics[:, -1, 1, ACCURACY_INDEX]), np.mean(fold_metrics[:, -1, 1, AUC_INDEX])   \n",
    "\n",
    "\n",
    "def external_validation_to_tensorboard(loss, auc, accuracy, f1, recall, dataloader_len):\n",
    "    writer.add_text('loss', \"{:.4f}\".format(loss/dataloader_len), 0)\n",
    "    writer.add_text('auc', \"{:.4f}\".format(auc), 0)\n",
    "    writer.add_text('accuracy',\"{:.4f}\".format(accuracy), 0)\n",
    "    writer.add_text('f1',\"{:.4f}\".format(f1), 0)\n",
    "    writer.add_text('recall',\"{:.4f}\".format(recall), 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_padding(data):\n",
    "    mask = np.all(np.array(data) != -np.Inf, axis=1)\n",
    "    return data[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "662\n",
      "331 331\n"
     ]
    }
   ],
   "source": [
    "# =========================================== Dataset ======================================================\n",
    "\n",
    "# Initialiation\n",
    "dataset = GeneExpressionDataset(DATASET_HDF5, LABELS[1])\n",
    "\n",
    "# Train/Test Split\n",
    "\n",
    "train_size = int(TRAIN_PERC * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "dataloader = data_utils.DataLoader(dataset, batch_size=1, num_workers=0, shuffle=True, pin_memory=True, collate_fn=gene_pad_collate_fn)\n",
    "pos, neg = 0, 0\n",
    "for batch_idx, (data, coords, labels, slide_id, case_id, _) in enumerate(dataloader):\n",
    "    # print(labels)\n",
    "    if labels[0] == 1:\n",
    "        pos += 1\n",
    "    elif labels[0] == 0:\n",
    "        neg += 1\n",
    "print(pos, neg)\n",
    "# -------------------------------------- Auxiliary Functions ---------------------------------------------\n",
    "\n",
    "def remove_padding(data):\n",
    "    mask = np.all(np.array(data) != -np.Inf, axis=1)\n",
    "    return data[mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================== Train Function ========================================\n",
    "\n",
    "# ------------------------------------ Train Functions ----------------------------------------\n",
    "\n",
    "# Train for a Single Batch\n",
    "def train_batch(model, data, labels):\n",
    "    losses, Y_probs, true_labels = [], [], []\n",
    "    num_nan = 0\n",
    "    train_error = 0\n",
    "    train_correct = 0\n",
    "    \n",
    "    for data_element, label in zip(data, labels):\n",
    "        if torch.isnan(data_element).any():\n",
    "            num_nan += 1\n",
    "            continue\n",
    "        data_element = remove_padding(data_element)\n",
    "        if cuda:\n",
    "            data_element, label = data_element.to(device), label.to(device)\n",
    "        \n",
    "        loss, attention, scores = model.calculate_objective(data_element, label)\n",
    "        error, Y_hat, Y_prob    = model.calculate_classification_error(data_element, label)\n",
    "\n",
    "        if torch.isnan(Y_prob).any():\n",
    "            num_nan += 1\n",
    "            continue\n",
    "        losses.append(loss)\n",
    "        train_error += error\n",
    "        train_correct += (Y_hat == label).sum().item()\n",
    "        Y_probs.append(Y_prob.item())\n",
    "        true_labels.append(label)\n",
    "\n",
    "    return losses, train_error, train_correct, Y_probs, true_labels, num_nan\n",
    "\n",
    "\n",
    "# Full epoch training\n",
    "def train_epoch_embeddings(model, dataloader, optimizer, epoch, fold=\"\"):\n",
    "    model.train()\n",
    "    train_loss, train_error, train_correct = 0., 0., 0.\n",
    "    Y_probs = []\n",
    "    true_labels = []\n",
    "    num_nan = 0\n",
    "    \n",
    "    for batch_idx, (data, coords, labels, slide_id, case_id, _) in enumerate(dataloader):\n",
    "        batch_losses, batch_error, batch_correct, batch_y_probs, batch_labels, batch_nan = train_batch(model, data, labels)\n",
    "        final_loss = torch.stack(batch_losses)\n",
    "        train_loss += final_loss.sum()\n",
    "        train_error += batch_error\n",
    "        train_correct += batch_correct\n",
    "        Y_probs += batch_y_probs\n",
    "        batch_nan += batch_nan\n",
    "        true_labels += batch_labels\n",
    "\n",
    "        l2_lambda = 0.001\n",
    "        l2_norm = sum(p.pow(2.0).sum()\n",
    "                        for p in model.parameters())\n",
    "\n",
    "        final_loss = final_loss.mean() #+ l2_norm * l2_lambda\n",
    "        \n",
    "        final_loss.backward()\n",
    "        #nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "    return train_loss, train_correct, Y_probs, true_labels, num_nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================== Validation Functions =====================================\n",
    "\n",
    "def valid_batch(model, data, slide_ids, case_ids, labels, augmentation_flags, score_list):\n",
    "    valid_loss, valid_error, valid_correct = 0, 0, 0\n",
    "    Y_probs, true_labels = [], []\n",
    "    num_nan = 0\n",
    "\n",
    "    for data_element, label, slide_id, is_aug in zip(data, labels, slide_ids, augmentation_flags):\n",
    "        if is_aug:\n",
    "            break\n",
    "        data_element = remove_padding(data_element)\n",
    "\n",
    "        if cuda:\n",
    "            data_element, label = data_element.to(device), label.to(device)\n",
    "      \n",
    "        loss, attention, scores = model.calculate_objective(data_element, label)\n",
    "        error, Y_hat, Y_prob    = model.calculate_classification_error(data_element, label)\n",
    "        \n",
    "        score_list[slide_id] = scores \n",
    "        if torch.any(torch.isnan(Y_prob)):\n",
    "            num_nan += 1\n",
    "            continue\n",
    "        valid_loss += loss.item()\n",
    "        valid_error += error\n",
    "        valid_correct += (Y_hat == label).sum().item()\n",
    "        Y_probs.append(Y_prob.item())\n",
    "        true_labels.append(label)\n",
    "\n",
    "    return valid_loss, valid_correct, Y_probs, true_labels,  num_nan, score_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def valid_epoch_embeddings(model, dataloader, epoch, fold=\"\"):\n",
    "    model.eval()\n",
    "    valid_loss, valid_error, valid_correct = 0., 0., 0\n",
    "    true_labels = []\n",
    "    Y_probs = []\n",
    "    num_nan = 0\n",
    "    score_list = {}\n",
    "\n",
    "    for batch_idx, (data, coords, labels, slide_ids, case_ids, aug_flags) in enumerate(dataloader):\n",
    "        batch_loss, batch_correct, batch_y_probs, batch_labels, batch_nan, score_list = valid_batch(model, data, slide_ids, case_ids, labels, aug_flags, score_list)\n",
    "        valid_loss += batch_loss\n",
    "        valid_correct += batch_correct\n",
    "        Y_probs += batch_y_probs\n",
    "        num_nan += batch_nan\n",
    "        true_labels += batch_labels\n",
    "        \n",
    "\n",
    "    if epoch == \"validation\" and USE_TENSORBOARD:\n",
    "        accuracy, auc, f1, recall = calculate_metrics(Y_probs, true_labels)\n",
    "        external_validation_to_tensorboard(valid_loss, auc, accuracy, f1, recall, len(dataloader))\n",
    "\n",
    "    return valid_loss, valid_correct, Y_probs, true_labels, score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================ Cross Validation ===============================\n",
    "\n",
    "def model_initialization(model_type):\n",
    "    model = model_type()\n",
    "    model = model.to(device)\n",
    "    model.apply(weights_init)\n",
    "    return model\n",
    "\n",
    "def cross_validation(model_type, dataset, optimizer_type, lr, weight_decay, batch_size=1, num_epochs=15):\n",
    "    history = {'train_loss': [], 'test_loss': [],'train_acc':[],'test_acc':[]}\n",
    "\n",
    "    fold_metrics = np.empty((KFOLD_SPLITS, num_epochs, 2, 6))\n",
    "\n",
    "\n",
    "    for fold, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(dataset)))):\n",
    "        model = model_initialization(model_type)\n",
    "        optimizer = optimizer_type(model.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=weight_decay)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[2,5,10], gamma=0.5)\n",
    "\n",
    "        print('Fold {}'.format(fold + 1))\n",
    "        train_sampler = data_utils.SubsetRandomSampler(train_idx)\n",
    "        test_sampler = data_utils.SubsetRandomSampler(val_idx)\n",
    "        train_loader = data_utils.DataLoader(dataset, batch_size, sampler=train_sampler, pin_memory=True, collate_fn=gene_pad_collate_fn)\n",
    "        test_loader = data_utils.DataLoader(dataset, batch_size, sampler=test_sampler, pin_memory=True, collate_fn=gene_pad_collate_fn)\n",
    "\n",
    "        train_auc_epochs, train_accuracy_epochs, train_loss_epochs = [], [], []\n",
    "        test_auc_epochs, test_accuracy_epochs, test_loss = [], [], []\n",
    "        \n",
    "        for epoch in range(0, num_epochs):\n",
    "            train_loss, train_correct, train_probs, train_true_labels, num_nan = train_epoch_embeddings(model,train_loader, optimizer, epoch, fold)\n",
    "            test_loss, test_correct, test_probs, test_true_labels, _  = valid_epoch_embeddings(model,test_loader, epoch, fold)\n",
    "            \n",
    "            train_accuracy, train_auc, train_f1, train_recall = calculate_metrics(train_probs, train_true_labels)\n",
    "            test_accuracy, test_auc, test_f1, test_recall = calculate_metrics(test_probs, test_true_labels)  \n",
    "\n",
    "\n",
    "            num_bags_per_sample = train_dataset[0][0].shape[0] \n",
    "\n",
    "            fold_metrics[fold, epoch, 0, LOSS_INDEX] = train_loss / (len(train_sampler) * num_bags_per_sample - num_nan)\n",
    "            fold_metrics[fold, epoch, 0, ACCURACY_INDEX] = train_accuracy\n",
    "            fold_metrics[fold, epoch, 0, AUC_INDEX] = train_auc\n",
    "            fold_metrics[fold, epoch, 0, F1_INDEX] = train_f1\n",
    "            fold_metrics[fold, epoch, 0, RECALL_INDEX] = train_recall\n",
    "            \n",
    "            fold_metrics[fold, epoch, 1, LOSS_INDEX] = test_loss / len(test_loader)\n",
    "            fold_metrics[fold, epoch, 1, ACCURACY_INDEX] = test_accuracy\n",
    "            fold_metrics[fold, epoch, 1, AUC_INDEX] = test_auc\n",
    "            fold_metrics[fold, epoch, 1, F1_INDEX] = test_f1\n",
    "            fold_metrics[fold, epoch, 1, RECALL_INDEX] = test_recall\n",
    "        \n",
    "            lr_scheduler.step()\n",
    "            \n",
    "    loss, accuracy, auc = metrics_to_tensorboard(fold_metrics)\n",
    "\n",
    "    return model, loss, accuracy, auc    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "loss 0.6917188495397568 \n",
      "auc 0.55\n",
      "External Validation Loss 0.6739078834092707\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory ../model_weights/gene_expression/BRCA_10x/baselines does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m writer\u001b[39m.\u001b[39mclose()\n\u001b[1;32m     12\u001b[0m \u001b[39m# Save Weights and Dataset Indices\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m torch\u001b[39m.\u001b[39;49msave(model\u001b[39m.\u001b[39;49mstate_dict(), MODEL_WEIGHTS_FILE)\n\u001b[1;32m     14\u001b[0m test_indices \u001b[39m=\u001b[39m test_dataset\u001b[39m.\u001b[39mindices\n\u001b[1;32m     15\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mtest_indices.pkl\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/.conda/envs/tese/lib/python3.10/site-packages/torch/serialization.py:440\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    437\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    439\u001b[0m \u001b[39mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 440\u001b[0m     \u001b[39mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[39mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    441\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[1;32m    442\u001b[0m         \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/tese/lib/python3.10/site-packages/torch/serialization.py:315\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    314\u001b[0m     container \u001b[39m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[0;32m--> 315\u001b[0m \u001b[39mreturn\u001b[39;00m container(name_or_buffer)\n",
      "File \u001b[0;32m~/.conda/envs/tese/lib/python3.10/site-packages/torch/serialization.py:288\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 288\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49mPyTorchFileWriter(\u001b[39mstr\u001b[39;49m(name)))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Parent directory ../model_weights/gene_expression/BRCA_10x/baselines does not exist."
     ]
    }
   ],
   "source": [
    "# Cross Validation\n",
    "model, loss, _, auc = cross_validation(model_type, train_dataset, optim.Adam, LR, REG, BATCH_SIZE, NUM_EPOCHS)\n",
    "\n",
    "print(\"loss\", loss, \"\\nauc\", auc)\n",
    "# External Validation\n",
    "dataloader = data_utils.DataLoader(test_dataset, batch_size=1, num_workers=0, shuffle=True, pin_memory=True, collate_fn=gene_pad_collate_fn)\n",
    "valid_loss, valid_correct, probs, true_labels, scores = valid_epoch_embeddings(model, dataloader, \"validation\")\n",
    "print(\"External Validation Loss\", valid_loss/len(dataloader))\n",
    "\n",
    "writer.close()\n",
    "\n",
    "# Save Weights and Dataset Indices\n",
    "torch.save(model.state_dict(), MODEL_WEIGHTS_FILE)\n",
    "test_indices = test_dataset.indices\n",
    "with open('test_indices.pkl', 'wb') as f:\n",
    "    pickle.dump(test_indices, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tese",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
