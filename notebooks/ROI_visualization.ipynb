{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "# Add the parent directory to the system path\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from model import Attention, GatedAttention, AdditiveAttention\n",
    "from WSI_dataloader import collate, BreastWSIDataset, BreastEmbeddingDataset\n",
    "\n",
    "# ==================================================================================\n",
    " \n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ============================== Torch Imports =====================================\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data_utils\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_TENSORBOARD = False\n",
    "\n",
    "DATASET_HDF5 = \"/media/mdastorage/breast_5x_dataset.h5\"\n",
    "MODEL_WEIGHTS_FILE = \"../model_weights/additiveAttentionMIL_aug2.pt\"\n",
    "SLIDE_DATA_FILE = \"../slide_data/breast.csv\"\n",
    "\n",
    "\n",
    "CUDA_DEVICE = \"cuda:0\"\n",
    "\n",
    "torch.cuda.init()\n",
    "torch.cuda.memory_summary(device=None, abbreviated=False)\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(CUDA_DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "786 786 786\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "Dataset Fetched!\n"
     ]
    }
   ],
   "source": [
    "dataset = BreastEmbeddingDataset(DATASET_HDF5)\n",
    "\n",
    "with open('../test_indices.pkl', 'rb') as f:\n",
    "    test_indices = pickle.load(f)\n",
    "\n",
    "test_dataset = torch.utils.data.Subset(dataset, test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AdditiveAttention().to(device)\n",
    "model.load_state_dict(torch.load(MODEL_WEIGHTS_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_labels(label, pos, neg):\n",
    "    if label == 0:\n",
    "        neg += 1\n",
    "    else:\n",
    "        pos += 1\n",
    "    return pos, neg \n",
    "\n",
    "\n",
    "# ========================================== Validation Functions =====================================================\n",
    "\n",
    "def valid_epoch_embeddings(model, dataloader, epoch, fold=\"\", get_scores=False):\n",
    "    model.eval()\n",
    "    valid_loss, valid_error, valid_correct = 0., 0., 0\n",
    "    probs, true_labels = [], []\n",
    "    pos, neg = 0, 0\n",
    "    nan_loss = False\n",
    "    num_aug = 0\n",
    "    attention_scores = {}\n",
    "    Y_hats = []\n",
    "\n",
    "    for batch_idx, (data, coords, label, path) in enumerate(dataloader):\n",
    "        bag_label = label[0]\n",
    "        pos, neg = count_labels(bag_label, pos, neg)\n",
    "        num_aug = data.shape[0]\n",
    "        embedding = data[0,:,:]\n",
    "        if cuda:\n",
    "            embedding, bag_label = embedding.to(device), label.to(device)\n",
    "\n",
    "        loss, attention_weights = model.calculate_objective(embedding, bag_label)\n",
    "        error, Y_hat, Y_prob = model.calculate_classification_error(embedding, bag_label)\n",
    "\n",
    "        if get_scores:\n",
    "            attention_scores[path[0]] = (coords[0], attention_weights)\n",
    "\n",
    "        valid_loss += loss.item()\n",
    "        valid_error += error\n",
    "        valid_correct += (Y_hat == bag_label).sum().item()\n",
    "        Y_hats.append(Y_hat)\n",
    "        probs.append(Y_prob.item())\n",
    "        true_labels.append(label[0].item())\n",
    "\n",
    "\n",
    "    if epoch == \"validation\" and USE_TENSORBOARD:\n",
    "        accuracy, auc, f1, recall = calculate_metrics(probs, true_labels)\n",
    "        writer.add_text('loss', \"{:.4f}\".format(valid_loss/len(dataloader)))\n",
    "        writer.add_text('auc', \"{:.4f}\".format(auc))\n",
    "        writer.add_text('accuracy',\"{:.4f}\".format(accuracy))\n",
    "        writer.add_text('f1',\"{:.4f}\".format(f1))\n",
    "        writer.add_text('recall',\"{:.4f}\".format(recall))\n",
    "\n",
    "\n",
    "    if get_scores:\n",
    "        return valid_loss, valid_correct, probs, true_labels, attention_scores, Y_hats\n",
    "    else:\n",
    "        return valid_loss, valid_correct, probs, true_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from math import log2\n",
    "\n",
    "#print(\"Attention\", attention_scores)\n",
    "\n",
    "def get_slide_levels(slide_ids, magnification, csv_filename):\n",
    "    '''\n",
    "    Given a set of slide ids and the pretended magnification, it returns the correct magnification levels, according to the csv file\n",
    "    '''\n",
    "    slide_levels = {}\n",
    "    slide_info = pd.read_csv(csv_filename)\n",
    "\n",
    "    for i in range(len(slide_info)):\n",
    "        if slide_info[\"id\"][i] not in slide_ids:\n",
    "            #print(\"1\", slide_info[\"id\"][i])\n",
    "            continue\n",
    "        if round(slide_info[\"mpp\"][i], 1) not in [0.5,0.2,0.3]:\n",
    "            print(\"2\", round(slide_info[\"mpp\"][i]))\n",
    "            continue\n",
    "        max_magnification = 10/slide_info[\"mpp\"][i]\n",
    "        levels_to_decrease = int(log2(max_magnification / magnification))\n",
    "        slide_levels[slide_info[\"id\"][i]] = slide_info[\"max_level\"][i] - levels_to_decrease\n",
    "\n",
    "    return slide_levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.gdc_api_utils import getTile\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib as mpl\n",
    "\n",
    "color_map = cm.jet\n",
    "\n",
    "\n",
    "\n",
    "def get_ROI_color(attention_scores, color, i):\n",
    "    #sorted_attention_scores, index = attention_scores.sort()\n",
    "    scores = attention_scores.cpu().detach().numpy()\n",
    "    normalized_scores = (scores - np.min(scores)) / (np.max(scores) - np.min(scores))\n",
    "    normalized_scores = scores - np.min(scores)\n",
    "    mean = np.mean(normalized_scores)\n",
    "\n",
    "    changed_scores = normalized_scores - mean\n",
    "    return changed_scores[i]\n",
    "\n",
    "\n",
    "def get_deviation_scores(attention_scores, i):\n",
    "    scores = attention_scores.cpu().detach().numpy()\n",
    "    mean = np.mean(scores)\n",
    "    deviation_scores = scores - mean\n",
    "    deviation_scores += np.min(deviation_scores)\n",
    "    normalized_dev_scores = (deviation_scores - np.min(deviation_scores)) / (np.max(deviation_scores) - np.min(deviation_scores))\n",
    "    return normalized_dev_scores.T[0][i]\n",
    "\n",
    "def getPixelsInThumbnail(slide_id, magnification_level, coords_scores, show_original=False):\n",
    "    img = getTile(slide_id, 9, 0, 0)\n",
    "    img = np.array(img).transpose((1,0,2))\n",
    "    \n",
    "    if show_original:\n",
    "        original_img = img.transpose((1,0,2)).copy()\n",
    "    \n",
    "    img_width = img.shape[0]\n",
    "    img_height = img.shape[1]\n",
    "    levels = magnification_level - 9 \n",
    "    num_tiles = 2**levels\n",
    "\n",
    "    tiles = (int(num_tiles*img_width/512)+1, int(num_tiles*img_height/512)+1)\n",
    "\n",
    "    pixels_per_patch = int(512/num_tiles)\n",
    "\n",
    "    coords = coords_scores[0]\n",
    "    scores = coords_scores[1]\n",
    "\n",
    "    for index in range(len(coords)):\n",
    "        pixel_x = int(coords[index][0] * pixels_per_patch)\n",
    "        pixel_y = int(coords[index][1] * pixels_per_patch)\n",
    "        pixels_to_color = img[pixel_x:pixel_x+pixels_per_patch, pixel_y:pixel_y+pixels_per_patch, :]\n",
    "        attention_score = get_deviation_scores(scores, index)\n",
    "        \n",
    "\n",
    "        color = color_map(attention_score)\n",
    "        np.multiply(pixels_to_color,  [color[0], color[1], color[2]], out=pixels_to_color, casting=\"unsafe\")\n",
    "\n",
    "    img = img.transpose((1,0,2))\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(1,2,1)\n",
    "    ax1.imshow(img)\n",
    "    if show_original:\n",
    "        ax2 = fig.add_subplot(1,2,2)\n",
    "        ax2.imshow(original_img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_ROIs(attention_scores, Y_hats, img_path, show_original=True):\n",
    "    index = 0\n",
    "    for path, scores in attention_scores.items():\n",
    "        pred = Y_hats[index]\n",
    "        index += 1\n",
    "        label = path[4]\n",
    "        slide_id = path[6:]\n",
    "        print(label)\n",
    "        print(\"Label:\", label, \"Predicted:\", pred)\n",
    "        print(slide_id)\n",
    "        slide_level = get_slide_levels([slide_id], 5, SLIDE_DATA_FILE)\n",
    "        getPixelsInThumbnail(slide_id, slide_level[slide_id], scores, show_original)\n",
    "    \n",
    "\n",
    "def visualize_attention_scores_dist(attention_scores, Y_hats, img_path):\n",
    "    index = 0\n",
    "    for path, scores in attention_scores.items():\n",
    "        pred = Y_hats[index]\n",
    "        index += 1\n",
    "        label = path[4]\n",
    "        slide_id = path[6:]\n",
    "    \n",
    "        plt.hist(scores[1][0].cpu().detach().numpy(), range=[0.0,1.0])\n",
    "        plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Label: 0 Predicted: tensor(0., device='cuda:0')\n",
      "945983a7-108a-4e77-941a-b65bdb6575eb\n"
     ]
    },
    {
     "ename": "UnidentifiedImageError",
     "evalue": "cannot identify image file <_io.BytesIO object at 0x7fe4b7d41170>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnidentifiedImageError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m dataloader \u001b[39m=\u001b[39m data_utils\u001b[39m.\u001b[39mDataLoader(test_dataset, batch_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, num_workers\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, pin_memory\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, collate_fn\u001b[39m=\u001b[39mcollate)\n\u001b[1;32m      3\u001b[0m _, _, _, _, attention_scores, Y_hats \u001b[39m=\u001b[39m valid_epoch_embeddings(model, dataloader, \u001b[39m\"\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m\"\u001b[39m, get_scores\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> 4\u001b[0m visualize_ROIs(attention_scores, Y_hats, \u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[10], line 12\u001b[0m, in \u001b[0;36mvisualize_ROIs\u001b[0;34m(attention_scores, Y_hats, img_path, show_original)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mprint\u001b[39m(slide_id)\n\u001b[1;32m     11\u001b[0m slide_level \u001b[39m=\u001b[39m get_slide_levels([slide_id], \u001b[39m5\u001b[39m, SLIDE_DATA_FILE)\n\u001b[0;32m---> 12\u001b[0m getPixelsInThumbnail(slide_id, slide_level[slide_id], scores, show_original)\n",
      "Cell \u001b[0;32mIn[7], line 29\u001b[0m, in \u001b[0;36mgetPixelsInThumbnail\u001b[0;34m(slide_id, magnification_level, coords_scores, show_original)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgetPixelsInThumbnail\u001b[39m(slide_id, magnification_level, coords_scores, show_original\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m---> 29\u001b[0m     img \u001b[39m=\u001b[39m getTile(slide_id, \u001b[39m9\u001b[39;49m, \u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m)\n\u001b[1;32m     30\u001b[0m     img \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(img)\u001b[39m.\u001b[39mtranspose((\u001b[39m1\u001b[39m,\u001b[39m0\u001b[39m,\u001b[39m2\u001b[39m))\n\u001b[1;32m     32\u001b[0m     \u001b[39mif\u001b[39;00m show_original:\n",
      "File \u001b[0;32m~/Model/notebooks/../utils/gdc_api_utils.py:175\u001b[0m, in \u001b[0;36mgetTile\u001b[0;34m(slide_id, magnification_lvl, coord_x, coord_y)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[39mwith\u001b[39;00m requests\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[1;32m    173\u001b[0m     slide_object \u001b[39m=\u001b[39m session\u001b[39m.\u001b[39mget(getTileEndpoint(slide_id, magnification_lvl, coord_x, coord_y), stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 175\u001b[0m \u001b[39mreturn\u001b[39;00m Image\u001b[39m.\u001b[39;49mopen(BytesIO(slide_object\u001b[39m.\u001b[39;49mcontent))\u001b[39m.\u001b[39mconvert(\u001b[39m'\u001b[39m\u001b[39mRGB\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/tese/lib/python3.10/site-packages/PIL/Image.py:3283\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3281\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(message)\n\u001b[1;32m   3282\u001b[0m msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcannot identify image file \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (filename \u001b[39mif\u001b[39;00m filename \u001b[39melse\u001b[39;00m fp)\n\u001b[0;32m-> 3283\u001b[0m \u001b[39mraise\u001b[39;00m UnidentifiedImageError(msg)\n",
      "\u001b[0;31mUnidentifiedImageError\u001b[0m: cannot identify image file <_io.BytesIO object at 0x7fe4b7d41170>"
     ]
    }
   ],
   "source": [
    "dataloader = data_utils.DataLoader(test_dataset, batch_size=1, num_workers=0, shuffle=False, pin_memory=True, collate_fn=collate)\n",
    "\n",
    "_, _, _, _, attention_scores, Y_hats = valid_epoch_embeddings(model, dataloader, \"validation\", get_scores=True)\n",
    "visualize_ROIs(attention_scores, Y_hats, \"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tese",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "65ed21f19add0e62cd601ab09ae40de4e05bbfa5bc74c0c632f06cef632a585a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
